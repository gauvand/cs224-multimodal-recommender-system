# Integrating Audio, Video, and Lyrics through Multi-Modal Attention-based GAT Framework for Enhanced Music Recommendations
Authors: Gaurav Anand, Samantha Liu, Yiwei Ou

Recent studies have underscored the importance of multi-modal data in enhancing recommendation performance. Liu et al. (2022) provide a comprehensive survey on multi-modal recommender systems, emphasizing the challenges and potential of integrating multimedia to capture user preferences effectively. To fill the research gap, this project aims to construct an advanced attention-based GNN framework that fuses multi-modal embeddings of the audio, images, and lyrics features synthetically to improve the accuracy and reliability of current music recommender systems. Generally, there are two challenges we need to address before achieving this goal. 
1) The first one is the dataset: since there are few public datasets that contain such comprehensive information on the multiple modalities from different sources, we may need to integrate multiple publicly available datasets from each of these modalities on our own by using some matching techniques between the datasets. 
2) The second challenge is the approach to choose to fuse these multi-modal embeddings and integrate them into a comprehensive GNN framework. 

Please review our Medium article to see how we tackled these challenges: [PLACEHOLDER]
